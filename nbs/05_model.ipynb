{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference\n",
    "\n",
    "> Fill in a module description hered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange\n",
    "\n",
    "from toolformer.api import BaseAPI\n",
    "from toolformer.utils import extract_api_content, extract_api_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ToolFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        apis: List[BaseAPI],\n",
    "        config: dict\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.apis = apis\n",
    "        self.config = config\n",
    "        self.is_calling_api: bool = False\n",
    "        \n",
    "        # TODO: make a config class contains token_id\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config[\"tokenizer\"][\"path\"])\n",
    "        self.tokenizer = tokenizer # TODO: remove after debug\n",
    "        \n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        self.api_start_token_id = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token_id = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_token_id = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "\n",
    "        self.eos_token_ids = tokenizer(\n",
    "            [\".\", \".\\n\\n\"],\n",
    "            return_tensors=\"pt\"\n",
    "        )[\"input_ids\"].squeeze()\n",
    "\n",
    "        # TODO: support batch\n",
    "        self.api_request_content: torch.Tensor = torch.tensor([])\n",
    "    \n",
    "    def _sampling(self, probs: TensorType[\"batch_size\", \"seq_len\"]) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "        return torch.argmax(probs, dim=-1)\n",
    "    \n",
    "    def execute_api(self, text_ids: TensorType[\"seq_len\"]) -> Optional[TensorType[\"seq_len\"]]:\n",
    "        \"\"\"Execute an API call.\"\"\"\n",
    "        text = self.tokenizer.decode(text_ids, skip_special_tokens=True)\n",
    "        api_name = extract_api_name(text, is_end_token=False)\n",
    "\n",
    "        if api_name is not None:\n",
    "            # find does apis contains the api_name\n",
    "            for api in self.apis:\n",
    "                if api.name == api_name:\n",
    "                    api_content = extract_api_content(text, api_name=api_name)\n",
    "                    api_output = api(api_content)\n",
    "                    return self.tokenizer(api_output, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        return None\n",
    "    \n",
    "    def add_idx_to_api_request_content(self, idx: TensorType[1]):\n",
    "        self.api_request_content = torch.cat([\n",
    "            self.api_request_content,\n",
    "            rearrange(idx, '... -> 1 ...')\n",
    "        ], dim=-1).long()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: TensorType[\"batch_size\", \"seq_len\"],\n",
    "        attention_mask: Optional[TensorType[\"batch_size\", \"seq_len\"]]=None,\n",
    "        max_new_tokens: int = 10,\n",
    "        **kwargs\n",
    "    ) -> TensorType[\"batch_size\", \"seq_len\"]:\n",
    "        # check padding to the left\n",
    "        generated_ids = input_ids\n",
    "        \n",
    "        for _ in range(max_new_tokens):\n",
    "            output_ids = self.model(\n",
    "                input_ids=generated_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            logits = output_ids.logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # TODO: k should be a config\n",
    "            _, top_k_idx = torch.topk(probs, k=1, dim=-1)\n",
    "            \n",
    "            if self.is_calling_api is True:\n",
    "                if self.api_end_token_id in top_k_idx:\n",
    "                    # if the api end token is in the top_k_idx, then we will execute the api\n",
    "                    # and then add api_end_token_id to the generated_ids\n",
    "                    # TODO: add support batch\n",
    "                    api_output_ids = self.execute_api(self.api_request_content[0])\n",
    "                    if api_output_ids is not None:\n",
    "                        pred_ids = torch.cat([\n",
    "                            self.api_output_token_id,\n",
    "                            api_output_ids,\n",
    "                            self.api_end_token_id\n",
    "                        ], dim=-1).long()\n",
    "                    else:\n",
    "                        pred_ids = self.api_end_token_id\n",
    "                    self.is_calling_api = False\n",
    "                else:\n",
    "                    pred_ids = self._sampling(probs)\n",
    "                    self.add_idx_to_api_request_content(pred_ids)\n",
    "            else:\n",
    "                if self.api_start_token_id in top_k_idx:\n",
    "                    # if the api start token is in the top_k_idx, then we are calling an api\n",
    "                    self.is_calling_api = True\n",
    "                    pred_ids = self.api_start_token_id\n",
    "                    self.add_idx_to_api_request_content(pred_ids)\n",
    "                else:\n",
    "                    pred_ids = self._sampling(probs)\n",
    "            \n",
    "            generated_ids = torch.cat([\n",
    "                generated_ids,\n",
    "                rearrange(pred_ids, '... -> 1 ...')\n",
    "            ], dim=1)\n",
    "            \n",
    "            attention_mask = torch.cat([\n",
    "                attention_mask,\n",
    "                rearrange(torch.ones_like(pred_ids), '... -> 1 ...')\n",
    "            ], dim=1)\n",
    "            \n",
    "            # ignore the case that pred_ids contains api_output\n",
    "            if len(pred_ids) == 1 and pred_ids in self.eos_token_ids:\n",
    "                break\n",
    "        \n",
    "        return generated_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
