{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator\n",
    "\n",
    "> Fill in a module description here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "from typing import List, Callable, Tuple, Union, TypedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtyping import TensorType\n",
    "from einops import rearrange\n",
    "\n",
    "from toolformer.api import BaseAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AugmentedCandidate(TypedDict):\n",
    "    api_start_positions: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataGenerator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        model: Callable, tokenizer: Callable,\n",
    "        apis: List[BaseAPI],\n",
    "        device: str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ):\n",
    "        super().__init__()\n",
    "        start_character = config[\"data_generator\"][\"api_start_character\"]\n",
    "        end_character = config[\"data_generator\"][\"api_end_character\"]\n",
    "        output_character = config[\"data_generator\"][\"api_output_character\"]\n",
    "        \n",
    "        # add a space, because when the model generate a token, it's also include a \"space\"\n",
    "        self.api_start_token_id = tokenizer(f' {start_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_end_token_id = tokenizer(end_character, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        self.api_output_token_id = tokenizer(f'{output_character}', return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "        self.top_k_sampling = config[\"data_generator\"][\"top_k_sampling\"]\n",
    "        self.sampling_threshold = config[\"data_generator\"][\"sampling_threshold\"]\n",
    "        self.filtering_threshold = config[\"data_generator\"][\"filtering_threshold\"]\n",
    "        \n",
    "        self.apis = apis\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        \n",
    "        # TODO: handle for cases that the sentence contains \".\\n\\n\"\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.eos_token_id = tokenizer(\".\\n\\n\")[\"input_ids\"][0]\n",
    "    \n",
    "    def sample_api_position(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"seq_len\"], # the ids of the prompt\n",
    "    ) -> Tuple[\n",
    "        TensorType[\"n_positions\"], # The positions of api call\n",
    "        TensorType[\"seq_len\"] # The generated text\n",
    "    ]:\n",
    "        \"\"\"Sampling API positions.\"\"\"\n",
    "        # TODO: add support batch\n",
    "        # the ids of the prompt and generated_ids\n",
    "        prompt_and_generated_ids = prompt_ids\n",
    "        # only the ids of the generated_ids\n",
    "        generated_ids = torch.tensor([]).to(self.device)\n",
    "        i = torch.tensor([0]).to(self.device)\n",
    "        \n",
    "        api_pos_probs = torch.tensor([])\n",
    "        \n",
    "        with torch.no_grad():    \n",
    "            while True:\n",
    "                logits = self.model(\n",
    "                    input_ids=prompt_and_generated_ids.unsqueeze(0),\n",
    "                ).logits\n",
    "\n",
    "                last_logit = logits[0, -1, :]\n",
    "                probs = torch.softmax(last_logit, dim=-1)\n",
    "                api_start_prob = probs[self.api_start_token_id]\n",
    "                \n",
    "                if api_start_prob > self.sampling_threshold:\n",
    "                    api_pos_probs = torch.cat([\n",
    "                        api_pos_probs,\n",
    "                        torch.tensor([api_start_prob, i]).unsqueeze(0)\n",
    "                    ], dim=0)     \n",
    "                \n",
    "                # sampling a token\n",
    "                # next_token = torch.multinomial(probs, num_samples=1)\n",
    "                next_token = torch.argmax(probs, dim=-1)\n",
    "                next_token = next_token.unsqueeze(0)\n",
    "                \n",
    "                prompt_and_generated_ids = torch.cat([prompt_and_generated_ids, next_token], dim=0)\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=0)\n",
    "                \n",
    "                if next_token == self.eos_token_id:\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        if api_pos_probs.numel() == 0:\n",
    "            api_positions = torch.tensor([])\n",
    "        else:\n",
    "            _, indices = torch.sort(api_pos_probs[:, 0], descending=True)\n",
    "            top_k_sampling = self.top_k_sampling\n",
    "            api_positions = api_pos_probs[indices[:top_k_sampling], 1]\n",
    "                    \n",
    "        return api_positions.long(), generated_ids.long()\n",
    "\n",
    "    def obtain_api_response(\n",
    "        self,\n",
    "        prompt_ids: TensorType[\"seq_len\"],\n",
    "        positions: TensorType[\"n_positions\"],\n",
    "        generated_ids: TensorType[\"seq_len\"]\n",
    "    ) -> TensorType[\"n_positions\", \"seq_len\"]:\n",
    "        \n",
    "        MAX_PAD = 50\n",
    "        \n",
    "        # the ids before the start of an api call\n",
    "        pre_api_ids = torch.tensor([])\n",
    "\n",
    "        for position in positions:\n",
    "            text_ids = torch.cat([generated_ids[:position], self.api_start_token_id], dim=0)\n",
    "            padded_text_ids = F.pad(text_ids, pad=(MAX_PAD - text_ids.shape[-1], 0), value=self.pad_token_id)\n",
    "            \n",
    "            pre_api_ids = torch.cat([\n",
    "                pre_api_ids,\n",
    "                rearrange(padded_text_ids, \"... -> 1 ...\")\n",
    "            ])\n",
    "        \n",
    "        PROMPT_LENGTH = len(prompt_ids)\n",
    "        \n",
    "        # TODO: optimzie this\n",
    "        prompt_and_pre_api_ids = torch.tensor([])\n",
    "        for x in pre_api_ids:\n",
    "            prompt_and_pre_api_ids = torch.cat([\n",
    "                prompt_and_pre_api_ids,\n",
    "                torch.cat([prompt_ids, x]).unsqueeze(0)\n",
    "            ], dim=0)\n",
    "                     \n",
    "        with torch.no_grad():\n",
    "            candidate_ids = self.model.generate(\n",
    "                input_ids=prompt_and_pre_api_ids.long(),\n",
    "                eos_token_id=self.eos_token_id,\n",
    "                max_new_tokens=50,\n",
    "            )\n",
    "        \n",
    "        # filter out the prompt template\n",
    "        # only keep the generated ids\n",
    "        candidate_ids = candidate_ids[:, PROMPT_LENGTH:]\n",
    "        \n",
    "        return candidate_ids\n",
    "    \n",
    "    def _generate_conditioning_prompts(\n",
    "        self,\n",
    "        api: BaseAPI,\n",
    "        candidate_ids: TensorType[\"n_candidates\", \"seq_len\"],\n",
    "    ):\n",
    "        conditioning_api_ids = torch.tensor([])\n",
    "\n",
    "        API_NAME = api.name\n",
    "        MAX_PAD = 100\n",
    "        \n",
    "        def extract_api_request_content(text: str, api_name: str) -> str:\n",
    "            \"\"\"Extract the content of an API request from a given text.\"\"\"\n",
    "            start_tag = f\"{api_name}(\"\n",
    "            end_tag = \")\"\n",
    "            start_idx = text.find(start_tag)\n",
    "            if start_idx == -1:\n",
    "                return None\n",
    "            start_idx += len(start_tag)\n",
    "            end_idx = text.find(end_tag, start_idx)\n",
    "            if end_idx == -1:\n",
    "                return None\n",
    "            return text[start_idx:end_idx]\n",
    "        \n",
    "        def extract_api_syntax(text: str, api_name: str) -> str:\n",
    "            \"\"\"Extract the API Syntax from a given text.\"\"\"\n",
    "            pattern = r\"\\[{}\\(.*?\\)\\]\".format(api_name)\n",
    "            matches = re.findall(pattern, text)\n",
    "            return matches\n",
    "\n",
    "        for text_ids in candidate_ids:\n",
    "            # the ids of the prediction\n",
    "            text = self.tokenizer.decode(text_ids, skip_special_tokens=True)\n",
    "            \n",
    "            api_request_content = extract_api_request_content(text, api_name=API_NAME)\n",
    "            api_response = api(api_request_content)\n",
    "            api_response_ids = self.tokenizer(api_response, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            # Format: \"-> [api_response]\"\n",
    "            api_response_with_arrow_ids = torch.cat([self.api_output_token_id, api_response_ids], dim=0)\n",
    "            \n",
    "            api_syntax = extract_api_syntax(text, api_name=API_NAME)\n",
    "            api_syntax_ids = self.tokenizer(api_syntax, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            api_syntax_with_response_ids = torch.cat([api_syntax_ids[:-1], api_response_with_arrow_ids, api_syntax_ids[-1:]])\n",
    "            api_syntax_without_response_ids = torch.cat([api_syntax_ids[:-1], self.api_output_token_id, api_syntax_ids[-1:]])\n",
    "                              \n",
    "            padded_api_without_response = rearrange(\n",
    "                F.pad(api_syntax_without_response_ids, pad=((MAX_PAD - api_syntax_without_response_ids.shape[-1]), 0), value=self.pad_token_id),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "            padded_api_with_response = rearrange(\n",
    "                F.pad(api_syntax_with_response_ids, pad=((MAX_PAD - api_syntax_with_response_ids.shape[-1]), 0), value=self.pad_token_id),\n",
    "                \"... -> 1 ...\"\n",
    "            )\n",
    "        \n",
    "            padded_api_call = torch.cat([\n",
    "                padded_api_without_response,\n",
    "                padded_api_with_response\n",
    "            ], dim=0)\n",
    "            padded_api_call = rearrange(padded_api_call, \"... -> 1 ...\")\n",
    "            \n",
    "            conditioning_api_ids = torch.cat([conditioning_api_ids, padded_api_call], dim=0).long()\n",
    "                    \n",
    "        return conditioning_api_ids\n",
    "\n",
    "    def _filter_candidate_by_threshold(\n",
    "        self,\n",
    "        losses,\n",
    "        candidates: TensorType[\"seq_len\"]\n",
    "    ):\n",
    "        filtered_augmented_text_ids = torch.tensor([])\n",
    "        for i, position in enumerate(losses):\n",
    "            negative_loss = min(losses[position][0], losses[position][1])\n",
    "            positive_loss = losses[position][2]\n",
    "            \n",
    "            if negative_loss - positive_loss >= self.filtering_threshold:\n",
    "                # filtered_augmented_text_ids.append(candidates[i])\n",
    "                filtered_augmented_text_ids = torch.cat([\n",
    "                    filtered_augmented_text_ids,\n",
    "                    candidates[i].unsqueeze(0)\n",
    "                ], dim=0)\n",
    "        \n",
    "        return filtered_augmented_text_ids.long()\n",
    "\n",
    "    def filter_api( \n",
    "        self,\n",
    "        api: BaseAPI,\n",
    "        text_ids: TensorType[\"seq_len\"],\n",
    "        api_start_idxs: TensorType[\"n_positions\"],\n",
    "        candidate_ids: TensorType[\"n_positions\", \"seq_len\"]\n",
    "    ):\n",
    "        conditioning_api_ids = self._generate_conditioning_prompts(api, candidate_ids)\n",
    "                \n",
    "        SPACE_TOKEN = self.tokenizer(\". \", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        API_LENGTH = 100\n",
    "        augmented_text_ids = {\"api_start_positions\": {}}\n",
    "        \n",
    "        def _compute_weight(t: int) -> Union[int, float]:\n",
    "            \"\"\"Compute the weight in the loss function.\"\"\"\n",
    "            return max(0, 1-0.2*t)\n",
    "        \n",
    "        for idx, api_ids in zip(api_start_idxs, conditioning_api_ids):\n",
    "            idx = idx.item()\n",
    "            seq_len = len(text_ids)\n",
    "            augmented_text_ids[\"api_start_positions\"][idx] = {\n",
    "                \"seq_positions\": {}\n",
    "            }\n",
    "\n",
    "            j = idx\n",
    "            while j <= seq_len - 1:\n",
    "                # if the model predic\n",
    "                if j == 1:\n",
    "                    j += 1\n",
    "                    continue\n",
    "                \n",
    "                # in the formua, from x_1 to x_j (include x_j)\n",
    "                # => generate_ids[:j]\n",
    "                conditioning_text_ids = text_ids[:j]\n",
    "                api_and_text_ids = torch.stack([\n",
    "                    F.pad(conditioning_text_ids, pad=(API_LENGTH + len(SPACE_TOKEN), 0), value=self.pad_token_id), # [text_ids]\n",
    "                    torch.cat([api_ids[0], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api->, text_ids]\n",
    "                    torch.cat([api_ids[1], SPACE_TOKEN, conditioning_text_ids], dim=0), # [api->result, text_ids]\n",
    "                ], dim=0)\n",
    "                                \n",
    "                # the next token after x_j\n",
    "                next_token_ids = text_ids[j]\n",
    "                augmented_text_ids[\"api_start_positions\"][idx][\"seq_positions\"][j] = {\n",
    "                    \"prompt_ids\": api_and_text_ids,\n",
    "                    \"unnormalized_weight\": _compute_weight(t=j-idx),\n",
    "                    \"losses\": [],\n",
    "                    \"target_ids\": torch.tensor([next_token_ids, next_token_ids, next_token_ids])\n",
    "                }\n",
    "                j += 1\n",
    "        \n",
    "        def _normalize_weights(augmented_text_ids):\n",
    "            \"\"\"Normalize the weight of each position in a sequence.\"\"\"\n",
    "            for api_start_position in augmented_text_ids[\"api_start_positions\"].values():\n",
    "                total_weight = sum([seq_position[\"unnormalized_weight\"] for seq_position in api_start_position[\"seq_positions\"].values()])\n",
    "                for seq_position in api_start_position[\"seq_positions\"].values():\n",
    "                    seq_position[\"normalized_weight\"] = seq_position[\"unnormalized_weight\"] / total_weight\n",
    "            \n",
    "            return augmented_text_ids\n",
    "        \n",
    "        augmented_text_ids = _normalize_weights(augmented_text_ids)\n",
    "                \n",
    "        def extract_conditioning_ids_and_target_ids(augmented_text_ids):\n",
    "            conditioning_text_ids = torch.tensor([])\n",
    "            target_ids = torch.tensor([])\n",
    "            \n",
    "            for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n",
    "                for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n",
    "                    target_ids = torch.concat([target_ids, seq_position_dict[\"target_ids\"]], dim=0)\n",
    "                    for prompt_id in seq_position_dict[\"prompt_ids\"]:\n",
    "                        conditioning_text_ids = torch.cat([\n",
    "                            conditioning_text_ids,\n",
    "                            F.pad(prompt_id.long(), pad=(50-prompt_id.shape[-1], 0), value=self.pad_token_id).unsqueeze(0)\n",
    "                        ], dim=0)\n",
    "        \n",
    "            return conditioning_text_ids.long(), target_ids.long()\n",
    "\n",
    "        conditioning_text_ids, target_ids = extract_conditioning_ids_and_target_ids(augmented_text_ids)\n",
    "            \n",
    "        output = self.model(input_ids=conditioning_text_ids.long())\n",
    "        logits = output.logits[:, -1, :]\n",
    "                    \n",
    "        def extract_target_logprob_from_logits(logits, target_ids):\n",
    "            log_probs = F.log_softmax(logits, dim=-1)\n",
    "            target_log_probs = log_probs[range(target_ids.shape[-1]), target_ids]\n",
    "            return target_log_probs\n",
    "\n",
    "        log_probs = extract_target_logprob_from_logits(logits, target_ids)\n",
    "            \n",
    "        for _, api_start_position_dict in augmented_text_ids[\"api_start_positions\"].items():\n",
    "            for _, seq_position_dict in api_start_position_dict[\"seq_positions\"].items():\n",
    "                seq_position_dict[\"losses\"] = log_probs[:3].squeeze(0)\n",
    "                log_probs = log_probs[3:]\n",
    "        \n",
    "        def _calculate_weighted_loss(augmented_text_ids):\n",
    "            for position in augmented_text_ids[\"api_start_positions\"]:        \n",
    "                seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n",
    "                for i in seq_positions:\n",
    "                    losses = seq_positions[i][\"losses\"]\n",
    "                    weights = seq_positions[i][\"normalized_weight\"]\n",
    "                    seq_positions[i][\"weighted_losses\"] = -losses * weights\n",
    "            \n",
    "            return augmented_text_ids\n",
    "        \n",
    "        augmented_text_ids = _calculate_weighted_loss(augmented_text_ids)\n",
    "        \n",
    "        def _calculate_loss(augmented_text_ids):\n",
    "            data = {}\n",
    "            for position in augmented_text_ids[\"api_start_positions\"]:        \n",
    "                seq_positions = augmented_text_ids[\"api_start_positions\"][position][\"seq_positions\"]\n",
    "                losses = [0, 0, 0]            \n",
    "                for i in seq_positions:\n",
    "                    losses[0] += seq_positions[i][\"weighted_losses\"][0] # loss for [text]\n",
    "                    losses[1] += seq_positions[i][\"weighted_losses\"][1] # loss for [api->, text]\n",
    "                    losses[2] += seq_positions[i][\"weighted_losses\"][2] # loss for [api-result, text]\n",
    "                data[position] = losses\n",
    "                \n",
    "            return data\n",
    "        \n",
    "        losses = _calculate_loss(augmented_text_ids)\n",
    "        filtered_candidate_ids = self._filter_candidate_by_threshold(losses, candidate_ids)\n",
    "        return filtered_candidate_ids\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "    ) -> TensorType[\"n_apis\", \"n_candidates\", \"seq_len\"]:\n",
    "        filtered_apis = torch.tensor([])\n",
    "        \n",
    "        for api in self.apis:\n",
    "            # TODO: add support batch\n",
    "            prompt = api.prompt_template.format(input=text)\n",
    "            prompt_ids = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "        \n",
    "            # sampling positions\n",
    "            api_start_idxs, generated_ids = self.sample_api_position(prompt_ids)\n",
    "            \n",
    "            # obtaining api responses\n",
    "            candidate_ids = self.obtain_api_response(prompt_ids, api_start_idxs, generated_ids)\n",
    "\n",
    "            # filtering\n",
    "            text_ids = self.tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "            \n",
    "            # return prompt_ids, api_start_idxs, generated_ids, candidate_ids, text_ids\n",
    "            filtered_candidate_ids = self.filter_api(api, text_ids, api_start_idxs, candidate_ids)\n",
    "                    \n",
    "            filtered_apis = torch.cat([filtered_apis, filtered_candidate_ids.unsqueeze(0)], dim=0)\n",
    "        \n",
    "        return filtered_apis.long()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
