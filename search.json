[
  {
    "objectID": "prompt.html",
    "href": "prompt.html",
    "title": "Prompts",
    "section": "",
    "text": "Calculator Prompt\n\n\nQuestion-Answering Prompt\n\n\nWolfram Alpha"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "üõ†Ô∏è ToolFormer (Pytorch)",
    "section": "",
    "text": "image.png\n\n\nPaper: Toolformer: Language Models Can Teach Themselves to Use Tools\nI‚Äôm implementing ToolFormer on stream. Wanna watch it in action? Go: twitch.tv/xrsrke\n\nTODO\n\n‚úÖ Augment data with batch size 1 :)\nSupport agument a batch of text\nExecute API calls in parallel\n‚úÖ Inference with batch size 1 :)\nInference with batch size more than 1 :)\nSupport Data Generation to GPU\nAdd ü§ó Accelerate\n\nAPI\n\n‚úÖ Support add custom API\n‚úÖ Calculator API\n‚úÖ WolframeAlpha API\n\n\n\nInstall\nInstall from PipPy\npip install toolformer\nInstall directly from the source code\ngit clone https://github.com/xrsrke/toolformer.git\ncd toolformer\npip install -e .\n\n\nü§ñ Data Generation\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom toolformer.data_generator import DataGenerator\nfrom toolformer.api import CalculatorAPI\nfrom toolformer.prompt import calculator_prompt\nfrom toolformer.utils import yaml2dict\n\n\nconfig = yaml2dict('../configs/default.yaml')\ncalculator_api = CalculatorAPI(\n    \"Calculator\", calculator_prompt,\n    sampling_threshold=0.2, filtering_threshold=0.2\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\"bigscience/bloom-560m\")\ntokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloom-560m\")\n\n\ntext = \"From this, we have 10 - 5 minutes = 5 minutes.\"\napis = [calculator_api]\ngenerator = DataGenerator(config, model, tokenizer, apis=apis)\n\n\naugumented_text_ids = generator.generate(text)\n\n\nprint(tokenizer.decode(augumented_text_ids[0][0], skip_special_tokens=True))\n\nFrom this, we have 10 - 5 minutes = [Calculator(10 - 5)] 5 minutes.\n\n\n\n\n\n\nAdd a Custom API?\nFirst, you need to add a new API.\nfrom toolformer.api import BaseAPI\n\nclass WikiSearchAPI(BaseAPI):\n    def execute(self, text):\n        # your custom api endpoint or whatever\n        output = YourCustomAPIEndPoint(text)\n        return output\nThen, create a new prompt template following the form. That‚Äôs all!\n\nfrom toolformer.prompt import calculator_prompt\nprint(calculator_prompt)\n\n\nYour task is to add calls to a Calculator API to a piece of text. The API call should help you get information required to complete the text. \n\nYou can call the API by writing \"Calculator(operation)!\" where \"operation\" is the type of calculation you want to perform. Here are some examples of API calls:\n\nInput: John has 5 apples and his friend gave him 3 more. John now has 8 apples.\nOuput: John has 5 apples and his friend gave him 3 more. John now has [Calculator(\"5 + 3\")] 8 apples.\n\nInput: Jane needs to divide 24 pieces of candy equally among 6 kids. Each kid will get 4 pieces of candy.\nOutput: Jane needs to divide 24 pieces of candy equally among 6 kids. Each kid will get [Calculator(24 / 6)] 4 pieces of candy.\n\nInput: From this, we have 4 * 30 minutes = 120 minutes.\nOutput: From this, we have 4 * 30 minutes = [Calculator(4 * 30)] 120 minutes.\n\nInput: {input}\nOutput:\n\n\n\n\n\nResources\nHere‚Äôre resources that i used to implement this\n\nHow to sampling APIs call: https://www.youtube.com/live/RLcr4bqGsEQ\nHow to calculate the loss: https://youtu.be/lQI9S5ngfHQ\n\n\n\nCitations\n@inproceedings{Schick2023ToolformerLM,\n    title   = {Toolformer: Language Models Can Teach Themselves to Use Tools},\n    author  = {Timo Schick and Jane Dwivedi-Yu and Roberto Dessi and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},\n    year    = {2023}\n}\n@misc{https://doi.org/10.48550/arxiv.2211.05100,\n  title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},  \n  author = {Workshop, BigScience and {:} and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Iliƒá, Suzana and Hesslow, Daniel and Castagn√©, Roman and Luccioni, Alexandra Sasha and Yvon, Fran√ßois and Gall√©, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Beno√Æt and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Lauren√ßon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo Gonz√°lez and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, G√©rard and Kruszewski, Germ√°n and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, J√∂rg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Mu√±oz, Manuel Romero and Masoud, Maraim and Grandury, Mar√≠a and ≈†a≈°ko, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and L√≥pez, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Ta≈üar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavall√©e, Pierre Fran√ßois and Lacroix, R√©mi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, St√©phane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and N√©v√©ol, Aur√©lie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdenƒõk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Mu√±oz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Cl√©mentine and Peri√±√°n, Daniel Le√≥n and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and P√†mies, Marc and Castillo, Maria A and Nezhurina, Marianna and S√§nger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Th√©o and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},title = {BLOOM: A 176B-Parameter Open-Access Multilingual Language Model},\n  year = {2022},\n}"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Util functions",
    "section": "",
    "text": "source\n\nyaml2dict\n\n yaml2dict (file_path)\n\n\nsource\n\n\nextract_api_content\n\n extract_api_content (text:str, api_name:str)\n\nExtract the content of an API request from a given text.\n\nsource\n\n\nextract_api_syntax\n\n extract_api_syntax (text:str, api_name:str)\n\nExtract the API Syntax from a given text.\n\nsource\n\n\nextract_api_name\n\n extract_api_name (text:str, is_end_token:bool=True)"
  },
  {
    "objectID": "api.html",
    "href": "api.html",
    "title": "APIs",
    "section": "",
    "text": "Base API\n\nsource\n\n\nBaseAPI\n\n BaseAPI (name:str,\n          prompt_template:langchain.prompts.prompt.PromptTemplate,\n          sampling_threshold:float=0.2, filtering_threshold:float=0.2)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nthe name of the API call\n\n\nprompt_template\nPromptTemplate\n\n\n\n\nsampling_threshold\nfloat\n0.2\n\n\n\nfiltering_threshold\nfloat\n0.2\n\n\n\n\n\n\nCalculator API\n\nsource\n\n\nCalculatorAPI\n\n CalculatorAPI (name:str,\n                prompt_template:langchain.prompts.prompt.PromptTemplate,\n                sampling_threshold:float=0.2,\n                filtering_threshold:float=0.2)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nname\nstr\n\nthe name of the API call\n\n\nprompt_template\nPromptTemplate\n\n\n\n\nsampling_threshold\nfloat\n0.2\n\n\n\nfiltering_threshold\nfloat\n0.2\n\n\n\n\n\n\nWolframe Alpha API\n\nsource\n\n\nWolframeAPI\n\n WolframeAPI (*args, api_key:str, **kargs)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "data_generator.html",
    "href": "data_generator.html",
    "title": "Data Generator",
    "section": "",
    "text": "source\n\nAugmentedCandidate\n\nsource\n\n\nDataGenerator\n\n DataGenerator (config:dict, model:Callable, tokenizer:Callable,\n                apis:List[toolformer.api.BaseAPI],\n                device:str=device(type='cpu'))\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Model Inference",
    "section": "",
    "text": "source\n\nToolFormer\n\n ToolFormer\n             (model:transformers.models.auto.modeling_auto.AutoModelForCau\n             salLM, apis:List[toolformer.api.BaseAPI], config:dict)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  }
]